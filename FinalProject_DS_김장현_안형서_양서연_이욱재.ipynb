{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c572b6d0",
   "metadata": {},
   "source": [
    "## 팀원\n",
    "- 김장현 / 컴퓨터공학부/ 2019-26471 \n",
    "- 안형서 / \n",
    "- 양서연 /\n",
    "- 이욱재 / \n",
    "\n",
    "## Intro \n",
    "- Here, we train neural networks solving the four Korean language tasks ([link](https://corpus.korean.go.kr/task/taskDownload.do?taskId=1&clCd=END_TASK&subMenuId=sub02)). \n",
    "- Our basic approach is to fine-tune pre-trained **korean language models** (https://huggingface.co/models?language=ko&sort=downloads). \n",
    "- We basically use **KLUE-RoBERTa** models from https://github.com/KLUE-benchmark/KLUE, which is the state-of-the-art korean language model. \n",
    "- We refer the following sources for the some parts of data processing and fine-tuning techniques. \n",
    " - Sun et al., 'How to Fine-Tune BERT for Text Classification?', https://arxiv.org/abs/1905.05583\n",
    " - https://github.com/NIKL-Team-BC/NIKL-KLUE\n",
    "- For more **detailed codes and experiment logs**, please refer to our [github page](https://github.com/codestella/nlp_final_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419dfa8",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344dea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import pandas\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "from transformers import AdamW\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "transformers.logging.set_verbosity(40) # Turn off warning\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcd4c3",
   "metadata": {},
   "source": [
    "## 1. 판정의문문 (BoolQ)\n",
    "\n",
    "아래 하이퍼파라미터 및 모델 세팅을 탐색 하였음. 가장 성능이 좋은 세팅은 강조되어 있음. 이 실험에 대한 보다 자세한 코드 및 로그파일은 [github page](https://github.com/codestella/nlp_final_project) 참조. \n",
    "- Model size: **Large** (\\~85%) / Base (\\~79%)\n",
    "- Epoch: 10\n",
    "- warm up: 10% training step (없으면 학습 불안정)\n",
    "- Learning rate: 1e-5, **8e-6**, 5e-6 (큰 차이는 없으나, 커지면 학습 불안정)\n",
    "- Batch size: **5**, 20, 60\n",
    "- Finetuning: **All**, Only classifier (i.e., freeze feature extractor, \\~57%)\n",
    "- Classifier: linear model 충분 (multi-layer로 늘려도 gain 작음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d331844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, tokenizer):\n",
    "    ''' Tokenization for BoolQ data'''\n",
    "    dataset = pandas.read_csv(path,\n",
    "                              delimiter='\\t',\n",
    "                              names=['ID', 'text', 'question', 'answer'],\n",
    "                              header=0)\n",
    "\n",
    "    tokenized = tokenizer(dataset['text'].tolist(),\n",
    "                          dataset['question'].tolist(),\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          return_tensors=\"pt\")\n",
    "    dataset['label'] = torch.tensor(dataset['answer'])\n",
    "    return dataset, tokenized\n",
    "\n",
    "\n",
    "class Roberta(RobertaModel):\n",
    "    ''' Classification layer added Roberta model'''\n",
    "    def __init__(self, config, model_name):\n",
    "        super(Roberta, self).__init__(config)\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name, config=config)\n",
    "        self.hdim = config.hidden_size\n",
    "        self.nclass = config.nclass\n",
    "        self.classifier = nn.Linear(self.hdim, self.nclass)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        h = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "354ba34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    ''' Define Torch Dataset '''\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        label = self.labels[idx]\n",
    "        return item, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a9887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer and model type\n",
    "model_type = \"Roberta\"\n",
    "size = 'large'\n",
    "model_name = f\"klue/roberta-{size}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa8b67",
   "metadata": {},
   "source": [
    "Set data path below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9d426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "base_path = './data'\n",
    "\n",
    "train_dataset, train_tokenized = load_data(os.path.join(base_path, 'SKT_BoolQ_Train.tsv'), tokenizer)\n",
    "val_dataset, val_tokenized = load_data(os.path.join(base_path, 'SKT_BoolQ_Dev.tsv'), tokenizer)\n",
    "\n",
    "train_dataset = TensorDataset(train_tokenized, train_dataset['label'])\n",
    "val_dataset = TensorDataset(val_tokenized, val_dataset['label'])\n",
    "\n",
    "# Define loader\n",
    "if size == 'base':\n",
    "    batch_size = 16\n",
    "else:\n",
    "    batch_size = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11004c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 로마 시대의 오리엔트의 범위는 제국 내에 동부 지방은 물론 제국 외부에 있는 다른 국가에 광범위하게 쓰이는 단어였다. 그 후에 로마 제국이 분열되고 서유럽이 그들의 중심적인 세계를 형성하는 과정에서 자신들을 옥시덴트 ( occident ), 서방이라 부르며 오리엔트는 이와 대조되는 문화를 가진 동방세계라는 뜻이 부가되어, 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어가 되었다. [SEP] 오리엔트는 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어로 쓰인다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data example\n",
    "tokenizer.decode(train_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a531b25",
   "metadata": {},
   "source": [
    "### 학습 결과 (BoolQ)\n",
    "- 최적 세팅에서 10번 반복한 결과 84% \\~ 87% 의 결과를 얻음\n",
    "- 각 실험은 1시간 정도 소요 (6min/epoch) \n",
    "- (참고) jupyter를 서버에서 돌려서 컴퓨터 연결이 끊겼을때 print가 잘 되지 않은 경우가 있으나, 모델 학습 및 저장은 이상 없음.\n",
    "- 본 모델을 앙상블하여 최종 **88.14\\%**의 validation accuracy 달성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa60a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, optimizer, scheduler):\n",
    "    ''' One epoch fine-tuning '''\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    cor = 0\n",
    "    n_sample = 0\n",
    "    s = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        item = {key: val.to(device) for key, val in data.items()}\n",
    "        target = target.to(device)\n",
    "\n",
    "        logits = model(**item)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        cor += (preds == target).sum().item()\n",
    "        n_sample += len(target)\n",
    "\n",
    "        print(f\"{cor}/{n_sample}\", end='\\r')\n",
    "\n",
    "    loss_avg = total_loss / n_sample\n",
    "    acc = cor / n_sample\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] Train loss: {loss_avg:.3f}, acc: {acc*100:.2f}, time: {time.time()-s:.1f}s\"\n",
    "    )\n",
    "    return acc\n",
    "\n",
    "\n",
    "def validate(epoch, model, val_loader, verbose=True):\n",
    "    ''' Evaluate on validation set '''\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    cor = 0\n",
    "    n_sample = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            item = {key: val.to(device) for key, val in data.items()}\n",
    "            target = target.to(device)\n",
    "\n",
    "            logits = model(**item)\n",
    "            loss = criterion(logits, target)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            pred_all.append(preds)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            cor += (preds == target).sum().item()\n",
    "            n_sample += len(target)\n",
    "\n",
    "    loss_avg = total_loss / n_sample\n",
    "    acc = cor / n_sample\n",
    "    pred_all = torch.cat(pred_all)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[Epoch {epoch}] Valid loss: {loss_avg:.3f}, acc: {acc*100:.2f}\")\n",
    "    return acc, pred_all\n",
    "\n",
    "\n",
    "def train(idx, num_epochs, lr, train_loader, val_loader, config, save_dir='./results'):\n",
    "    ''' Train for multiple epochs and validate '''    \n",
    "    print(f\"Start trining {idx}th model\")\n",
    "    model = Roberta(config, model_name).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = transformers.get_scheduler(\"linear\",\n",
    "                                           optimizer=optimizer,\n",
    "                                           num_warmup_steps=num_epochs * len(train_loader) // 10,\n",
    "                                           num_training_steps=num_epochs * len(train_loader))\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc = train_epoch(epoch, model, train_loader, optimizer, scheduler)\n",
    "        val_acc, _ = validate(epoch, model, val_loader)\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.save_pretrained(os.path.join(save_dir, f'{idx}'))\n",
    "            \n",
    "    print(f\"Training finish! Best validation accuracy: {best_acc*100:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa05a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ensemble(save_dir, model_name, val_loader, answer, idx_max=10, acc_threshold=0.85):\n",
    "    ''' Measure ensemble accuracy '''\n",
    "    pred_ensemble = []\n",
    "    for idx in range(idx_max):\n",
    "        model = Roberta.from_pretrained(os.path.join(save_dir, f'{idx}'), model_name)\n",
    "        model.to(device)\n",
    "        acc, pred_all = validate('best', model, val_loader, verbose=False)\n",
    "        print(f\"Load {idx}th model (acc: {acc*100:.2f})\")\n",
    "        if acc >= acc_threshold:\n",
    "            pred_ensemble.append(pred_all)\n",
    "        \n",
    "    pred_ensemble = torch.stack(pred_ensemble, dim=-1).float()\n",
    "    pred_ensemble = (pred_ensemble.mean(-1) >= 0.5).long().to(answer.device)\n",
    "    acc_ensemble = (pred_ensemble == answer).sum() / len(answer)\n",
    "    print(f\"\\nEnsemble accuracy: {acc_ensemble*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17295cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trining 0th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.135, acc: 55.66, time: 346.7s\n",
      "[Epoch 0] Valid loss: 0.111, acc: 72.00\n",
      "[Epoch 1] Train loss: 0.086, acc: 81.53, time: 348.5s\n",
      "[Epoch 1] Valid loss: 0.090, acc: 78.71\n",
      "[Epoch 2] Train loss: 0.037, acc: 93.40, time: 350.1s\n",
      "[Epoch 2] Valid loss: 0.094, acc: 82.57\n",
      "[Epoch 3] Train loss: 0.016, acc: 97.33, time: 349.0s\n",
      "[Epoch 3] Valid loss: 0.112, acc: 84.00\n",
      "[Epoch 4] Train loss: 0.006, acc: 98.99, time: 347.3s\n",
      "[Epoch 4] Valid loss: 0.167, acc: 84.00\n",
      "[Epoch 5] Train loss: 0.004, acc: 99.40, time: 350.4s\n",
      "[Epoch 5] Valid loss: 0.165, acc: 84.00\n",
      "[Epoch 6] Train loss: 0.003, acc: 99.48, time: 350.7s\n",
      "[Epoch 6] Valid loss: 0.144, acc: 87.00\n",
      "[Epoch 7] Train loss: 0.001, acc: 99.81, time: 349.5s\n",
      "[Epoch 7] Valid loss: 0.181, acc: 85.71\n",
      "[Epoch 8] Train loss: 0.001, acc: 99.89, time: 353.4s\n",
      "[Epoch 8] Valid loss: 0.189, acc: 85.00\n",
      "[Epoch 9] Train loss: 0.001, acc: 99.89, time: 350.9s\n",
      "[Epoch 9] Valid loss: 0.193, acc: 84.43\n",
      "Training finish! Best validation accuracy: 87.00\n",
      "\n",
      "Start trining 1th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.130, acc: 59.15, time: 351.2s\n",
      "[Epoch 0] Valid loss: 0.101, acc: 75.86\n",
      "[Epoch 1] Train loss: 0.081, acc: 82.18, time: 349.3s\n",
      "[Epoch 1] Valid loss: 0.070, acc: 84.71\n",
      "[Epoch 2] Train loss: 0.031, acc: 94.22, time: 350.0s\n",
      "[Epoch 2] Valid loss: 0.126, acc: 84.71\n",
      "[Epoch 3] Train loss: 0.013, acc: 97.93, time: 354.0s\n",
      "[Epoch 3] Valid loss: 0.132, acc: 84.29\n",
      "[Epoch 4] Train loss: 0.007, acc: 99.07, time: 353.1s\n",
      "[Epoch 4] Valid loss: 0.152, acc: 84.57\n",
      "[Epoch 5] Train loss: 0.002, acc: 99.54, time: 353.0s\n",
      "[Epoch 5] Valid loss: 0.164, acc: 84.71\n",
      "[Epoch 6] Train loss: 0.003, acc: 99.54, time: 352.5s\n",
      "[Epoch 6] Valid loss: 0.150, acc: 86.14\n",
      "[Epoch 7] Train loss: 0.002, acc: 99.73, time: 351.3s\n",
      "[Epoch 7] Valid loss: 0.168, acc: 85.71\n",
      "[Epoch 8] Train loss: 0.001, acc: 99.97, time: 354.4s\n",
      "[Epoch 8] Valid loss: 0.179, acc: 85.29\n",
      "[Epoch 9] Train loss: 0.001, acc: 99.95, time: 353.6s\n",
      "[Epoch 9] Valid loss: 0.179, acc: 85.71\n",
      "Training finish! Best validation accuracy: 86.14\n",
      "\n",
      "Start trining 2th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.140, acc: 50.97, time: 353.7s\n",
      "[Epoch 0] Valid loss: 0.134, acc: 59.14\n",
      "[Epoch 1] Train loss: 0.104, acc: 73.29, time: 353.2s\n",
      "[Epoch 1] Valid loss: 0.083, acc: 81.14\n",
      "3170/3485\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.006, acc: 99.13, time: 352.3s\n",
      "[Epoch 4] Valid loss: 0.148, acc: 83.00\n",
      "[Epoch 5] Train loss: 0.004, acc: 99.29, time: 354.9s\n",
      "[Epoch 5] Valid loss: 0.143, acc: 83.43\n",
      "[Epoch 6] Train loss: 0.003, acc: 99.56, time: 353.8s\n",
      "[Epoch 6] Valid loss: 0.193, acc: 82.86\n",
      "[Epoch 7] Train loss: 0.000, acc: 99.95, time: 356.7s\n",
      "[Epoch 7] Valid loss: 0.202, acc: 84.71\n",
      "425/425\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 0.002, acc: 99.70, time: 353.3s\n",
      "[Epoch 5] Valid loss: 0.179, acc: 85.00\n",
      "[Epoch 6] Train loss: 0.002, acc: 99.59, time: 353.9s\n",
      "[Epoch 6] Valid loss: 0.188, acc: 84.86\n",
      "[Epoch 7] Train loss: 0.001, acc: 99.84, time: 355.2s\n",
      "[Epoch 7] Valid loss: 0.184, acc: 85.00\n",
      "[Epoch 8] Train loss: 0.001, acc: 99.92, time: 355.2s\n",
      "[Epoch 8] Valid loss: 0.184, acc: 85.86\n",
      "3584/3585\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 0.035, acc: 94.00, time: 354.7s\n",
      "[Epoch 2] Valid loss: 0.100, acc: 82.00\n",
      "[Epoch 3] Train loss: 0.012, acc: 98.25, time: 357.2s\n",
      "[Epoch 3] Valid loss: 0.128, acc: 83.57\n",
      "[Epoch 4] Train loss: 0.007, acc: 98.80, time: 354.8s\n",
      "[Epoch 4] Valid loss: 0.150, acc: 82.57\n",
      "[Epoch 5] Train loss: 0.005, acc: 98.99, time: 356.3s\n",
      "[Epoch 5] Valid loss: 0.187, acc: 81.43\n",
      "1705/1705\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 0.002, acc: 99.75, time: 358.5s\n",
      "[Epoch 7] Valid loss: 0.181, acc: 83.57\n",
      "[Epoch 8] Train loss: 0.001, acc: 99.92, time: 358.0s\n",
      "[Epoch 8] Valid loss: 0.193, acc: 82.57\n",
      "[Epoch 9] Train loss: 0.000, acc: 99.97, time: 356.7s\n",
      "[Epoch 9] Valid loss: 0.209, acc: 81.86\n",
      "Training finish! Best validation accuracy: 83.57\n",
      "\n",
      "Start trining 5th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.140, acc: 51.24, time: 357.3s\n",
      "[Epoch 0] Valid loss: 0.123, acc: 68.14\n",
      "2083/2690\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.001, acc: 99.95, time: 359.1s\n",
      "[Epoch 9] Valid loss: 0.218, acc: 84.43\n",
      "Training finish! Best validation accuracy: 84.86\n",
      "\n",
      "Start trining 6th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.137, acc: 53.12, time: 359.3s\n",
      "[Epoch 0] Valid loss: 0.098, acc: 76.57\n",
      "[Epoch 1] Train loss: 0.086, acc: 81.17, time: 354.8s\n",
      "[Epoch 1] Valid loss: 0.086, acc: 81.86\n",
      "[Epoch 2] Train loss: 0.033, acc: 94.00, time: 358.5s\n",
      "[Epoch 2] Valid loss: 0.113, acc: 81.14\n",
      "2626/2690\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 0.002, acc: 99.86, time: 356.8s\n",
      "[Epoch 6] Valid loss: 0.181, acc: 84.57\n",
      "[Epoch 7] Train loss: 0.001, acc: 99.89, time: 358.8s\n",
      "[Epoch 7] Valid loss: 0.199, acc: 84.57\n",
      "[Epoch 8] Train loss: 0.000, acc: 99.95, time: 357.7s\n",
      "[Epoch 8] Valid loss: 0.205, acc: 85.29\n",
      "[Epoch 9] Train loss: 0.000, acc: 100.00, time: 355.7s\n",
      "[Epoch 9] Valid loss: 0.209, acc: 85.43\n",
      "Training finish! Best validation accuracy: 85.43\n",
      "\n",
      "Start trining 7th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/780\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 0.085, acc: 80.90, time: 355.3s\n",
      "[Epoch 1] Valid loss: 0.097, acc: 81.00\n",
      "[Epoch 2] Train loss: 0.037, acc: 93.70, time: 356.8s\n",
      "[Epoch 2] Valid loss: 0.088, acc: 84.57\n",
      "[Epoch 3] Train loss: 0.013, acc: 98.25, time: 355.4s\n",
      "[Epoch 3] Valid loss: 0.115, acc: 84.57\n",
      "[Epoch 4] Train loss: 0.009, acc: 98.64, time: 357.4s\n",
      "[Epoch 4] Valid loss: 0.121, acc: 85.00\n",
      "2834/2855\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 0.015, acc: 97.44, time: 357.1s\n",
      "[Epoch 3] Valid loss: 0.130, acc: 83.71\n",
      "[Epoch 4] Train loss: 0.006, acc: 99.02, time: 356.7s\n",
      "[Epoch 4] Valid loss: 0.135, acc: 84.29\n",
      "[Epoch 5] Train loss: 0.003, acc: 99.48, time: 356.5s\n",
      "[Epoch 5] Valid loss: 0.177, acc: 84.00\n",
      "[Epoch 6] Train loss: 0.001, acc: 99.95, time: 359.4s\n",
      "[Epoch 6] Valid loss: 0.232, acc: 84.00\n",
      "3238/3245\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.001, acc: 99.86, time: 361.4s\n",
      "[Epoch 9] Valid loss: 0.224, acc: 83.71\n",
      "Training finish! Best validation accuracy: 84.29\n",
      "\n",
      "Start trining 9th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.141, acc: 51.65, time: 359.3s\n",
      "[Epoch 0] Valid loss: 0.153, acc: 46.57\n",
      "[Epoch 1] Train loss: 0.104, acc: 74.05, time: 357.5s\n",
      "[Epoch 1] Valid loss: 0.088, acc: 79.43\n",
      "[Epoch 2] Train loss: 0.048, acc: 90.86, time: 356.8s\n",
      "[Epoch 2] Valid loss: 0.084, acc: 82.71\n",
      "[Epoch 3] Train loss: 0.017, acc: 97.57, time: 357.0s\n",
      "[Epoch 3] Valid loss: 0.150, acc: 79.57\n",
      "25/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.007, acc: 98.83, time: 360.3s\n",
      "[Epoch 4] Valid loss: 0.162, acc: 83.14\n",
      "[Epoch 5] Train loss: 0.004, acc: 99.32, time: 360.5s\n",
      "[Epoch 5] Valid loss: 0.185, acc: 83.57\n",
      "[Epoch 6] Train loss: 0.004, acc: 99.51, time: 360.3s\n",
      "[Epoch 6] Valid loss: 0.161, acc: 84.29\n",
      "[Epoch 7] Train loss: 0.003, acc: 99.62, time: 360.3s\n",
      "[Epoch 7] Valid loss: 0.164, acc: 83.86\n",
      "1589/1590\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning networks (10 repeat)\n",
    "lr = 8e-6\n",
    "num_epochs = 10\n",
    "save_dir = './results_qa'\n",
    "\n",
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "config.nclass = 2\n",
    "for i in range(10):\n",
    "    train(i, num_epochs, lr, train_loader, val_loader, config=config, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072f0916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 0th model (acc: 87.00)\n",
      "Load 1th model (acc: 86.14)\n",
      "Load 2th model (acc: 85.43)\n",
      "Load 3th model (acc: 85.86)\n",
      "Load 4th model (acc: 83.57)\n",
      "Load 5th model (acc: 84.86)\n",
      "Load 6th model (acc: 85.43)\n",
      "Load 7th model (acc: 85.43)\n",
      "Load 8th model (acc: 84.29)\n",
      "Load 9th model (acc: 84.29)\n",
      "\n",
      "Ensemble accuracy: 88.14\n"
     ]
    }
   ],
   "source": [
    "answer = torch.tensor(val_dataset.labels)\n",
    "save_dir = './results_qa'\n",
    "\n",
    "validate_ensemble(save_dir, model_name, val_loader, answer, idx_max=10, acc_threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b89b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
