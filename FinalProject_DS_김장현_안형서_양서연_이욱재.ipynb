{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 팀원\n",
    "- 김장현 / 컴퓨터공학부/ 2019-26471 \n",
    "- 안형서 / 전기정보공학부 / 2017-12932\n",
    "- 양서연 /\n",
    "- 이욱재 / \n",
    "\n",
    "## Intro \n",
    "- Here, we train neural networks solving the four Korean language tasks ([link](https://corpus.korean.go.kr/task/taskDownload.do?taskId=1&clCd=END_TASK&subMenuId=sub02)). \n",
    "- Our basic approach is to fine-tune pre-trained **korean language models** (https://huggingface.co/models?language=ko&sort=downloads). \n",
    "- We basically use **KLUE-RoBERTa** models from https://github.com/KLUE-benchmark/KLUE, which is the state-of-the-art korean language model. \n",
    "- We refer the following sources for the some parts of data processing and fine-tuning techniques. \n",
    " - Sun et al., 'How to Fine-Tune BERT for Text Classification?', https://arxiv.org/abs/1905.05583\n",
    " - https://github.com/NIKL-Team-BC/NIKL-KLUE\n",
    "- For more **detailed codes and experiment logs**, please refer to our [github page](https://github.com/codestella/nlp_final_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import pandas\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "from transformers import AdamW\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "transformers.logging.set_verbosity(40) # Turn off warning\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 판정의문문 (BoolQ)\n",
    "\n",
    "아래 하이퍼파라미터 및 모델 세팅을 탐색 하였음. 가장 성능이 좋은 세팅은 강조되어 있음. 이 실험에 대한 보다 자세한 코드 및 로그파일은 [github page](https://github.com/codestella/nlp_final_project) 참조. \n",
    "- Model size: **Large** (\\~85%) / Base (\\~79%)\n",
    "- Epoch: 10\n",
    "- warm up: 10% training step (없으면 학습 불안정)\n",
    "- Learning rate: 1e-5, **8e-6**, 5e-6 (큰 차이는 없으나, 커지면 학습 불안정)\n",
    "- Batch size: **5**, 20, 60\n",
    "- Finetuning: **All**, Only classifier (i.e., freeze feature extractor, \\~57%)\n",
    "- Classifier: linear model 충분 (multi-layer로 늘려도 gain 작음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, tokenizer):\n",
    "    ''' Tokenization for BoolQ data'''\n",
    "    dataset = pandas.read_csv(path,\n",
    "                              delimiter='\\t',\n",
    "                              names=['ID', 'text', 'question', 'answer'],\n",
    "                              header=0)\n",
    "\n",
    "    tokenized = tokenizer(dataset['text'].tolist(),\n",
    "                          dataset['question'].tolist(),\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          return_tensors=\"pt\")\n",
    "    dataset['label'] = torch.tensor(dataset['answer'])\n",
    "    return dataset, tokenized\n",
    "\n",
    "\n",
    "class Roberta(RobertaModel):\n",
    "    ''' Classification layer added Roberta model'''\n",
    "    def __init__(self, config, model_name):\n",
    "        super(Roberta, self).__init__(config)\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name, config=config)\n",
    "        self.hdim = config.hidden_size\n",
    "        self.nclass = config.nclass\n",
    "        self.classifier = nn.Linear(self.hdim, self.nclass)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        h = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    ''' Define Torch Dataset '''\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        label = self.labels[idx]\n",
    "        return item, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer and model type\n",
    "model_type = \"Roberta\"\n",
    "size = 'large'\n",
    "model_name = f\"klue/roberta-{size}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data path below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "base_path = './data'\n",
    "\n",
    "train_dataset, train_tokenized = load_data(os.path.join(base_path, 'SKT_BoolQ_Train.tsv'), tokenizer)\n",
    "val_dataset, val_tokenized = load_data(os.path.join(base_path, 'SKT_BoolQ_Dev.tsv'), tokenizer)\n",
    "\n",
    "train_dataset = TensorDataset(train_tokenized, train_dataset['label'])\n",
    "val_dataset = TensorDataset(val_tokenized, val_dataset['label'])\n",
    "\n",
    "# Define loader\n",
    "if size == 'base':\n",
    "    batch_size = 16\n",
    "else:\n",
    "    batch_size = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 로마 시대의 오리엔트의 범위는 제국 내에 동부 지방은 물론 제국 외부에 있는 다른 국가에 광범위하게 쓰이는 단어였다. 그 후에 로마 제국이 분열되고 서유럽이 그들의 중심적인 세계를 형성하는 과정에서 자신들을 옥시덴트 ( occident ), 서방이라 부르며 오리엔트는 이와 대조되는 문화를 가진 동방세계라는 뜻이 부가되어, 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어가 되었다. [SEP] 오리엔트는 인도와 중국, 일본을 이루는 광범위한 지역을 지칭하는 단어로 쓰인다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data example\n",
    "tokenizer.decode(train_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 결과 (BoolQ)\n",
    "- 최적 세팅에서 10번 반복한 결과 84% \\~ 87% 의 결과를 얻음\n",
    "- 각 실험은 1시간 정도 소요 (6min/epoch) \n",
    "- (참고) jupyter를 서버에서 돌려서 컴퓨터 연결이 끊겼을때 print가 잘 되지 않은 경우가 있으나, 모델 학습 및 저장은 이상 없음.\n",
    "- 본 모델을 앙상블하여 최종 **88.14\\%**의 validation accuracy 달성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, optimizer, scheduler):\n",
    "    ''' One epoch fine-tuning '''\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    cor = 0\n",
    "    n_sample = 0\n",
    "    s = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        item = {key: val.to(device) for key, val in data.items()}\n",
    "        target = target.to(device)\n",
    "\n",
    "        logits = model(**item)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        cor += (preds == target).sum().item()\n",
    "        n_sample += len(target)\n",
    "\n",
    "        print(f\"{cor}/{n_sample}\", end='\\r')\n",
    "\n",
    "    loss_avg = total_loss / n_sample\n",
    "    acc = cor / n_sample\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] Train loss: {loss_avg:.3f}, acc: {acc*100:.2f}, time: {time.time()-s:.1f}s\"\n",
    "    )\n",
    "    return acc\n",
    "\n",
    "\n",
    "def validate(epoch, model, val_loader, verbose=True):\n",
    "    ''' Evaluate on validation set '''\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    cor = 0\n",
    "    n_sample = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            item = {key: val.to(device) for key, val in data.items()}\n",
    "            target = target.to(device)\n",
    "\n",
    "            logits = model(**item)\n",
    "            loss = criterion(logits, target)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            pred_all.append(preds)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            cor += (preds == target).sum().item()\n",
    "            n_sample += len(target)\n",
    "\n",
    "    loss_avg = total_loss / n_sample\n",
    "    acc = cor / n_sample\n",
    "    pred_all = torch.cat(pred_all)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[Epoch {epoch}] Valid loss: {loss_avg:.3f}, acc: {acc*100:.2f}\")\n",
    "    return acc, pred_all\n",
    "\n",
    "\n",
    "def train(idx, num_epochs, lr, train_loader, val_loader, config, save_dir='./results'):\n",
    "    ''' Train for multiple epochs and validate '''    \n",
    "    print(f\"Start trining {idx}th model\")\n",
    "    model = Roberta(config, model_name).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = transformers.get_scheduler(\"linear\",\n",
    "                                           optimizer=optimizer,\n",
    "                                           num_warmup_steps=num_epochs * len(train_loader) // 10,\n",
    "                                           num_training_steps=num_epochs * len(train_loader))\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc = train_epoch(epoch, model, train_loader, optimizer, scheduler)\n",
    "        val_acc, _ = validate(epoch, model, val_loader)\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.save_pretrained(os.path.join(save_dir, f'{idx}'))\n",
    "            \n",
    "    print(f\"Training finish! Best validation accuracy: {best_acc*100:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ensemble(save_dir, model_name, val_loader, answer, idx_max=10, acc_threshold=0.85):\n",
    "    ''' Measure ensemble accuracy '''\n",
    "    pred_ensemble = []\n",
    "    for idx in range(idx_max):\n",
    "        model = Roberta.from_pretrained(os.path.join(save_dir, f'{idx}'), model_name)\n",
    "        model.to(device)\n",
    "        acc, pred_all = validate('best', model, val_loader, verbose=False)\n",
    "        print(f\"Load {idx}th model (acc: {acc*100:.2f})\")\n",
    "        if acc >= acc_threshold:\n",
    "            pred_ensemble.append(pred_all)\n",
    "        \n",
    "    pred_ensemble = torch.stack(pred_ensemble, dim=-1).float()\n",
    "    pred_ensemble = (pred_ensemble.mean(-1) >= 0.5).long().to(answer.device)\n",
    "    acc_ensemble = (pred_ensemble == answer).sum() / len(answer)\n",
    "    print(f\"\\nEnsemble accuracy: {acc_ensemble*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trining 0th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.135, acc: 55.66, time: 346.7s\n",
      "[Epoch 0] Valid loss: 0.111, acc: 72.00\n",
      "[Epoch 1] Train loss: 0.086, acc: 81.53, time: 348.5s\n",
      "[Epoch 1] Valid loss: 0.090, acc: 78.71\n",
      "[Epoch 2] Train loss: 0.037, acc: 93.40, time: 350.1s\n",
      "[Epoch 2] Valid loss: 0.094, acc: 82.57\n",
      "[Epoch 3] Train loss: 0.016, acc: 97.33, time: 349.0s\n",
      "[Epoch 3] Valid loss: 0.112, acc: 84.00\n",
      "[Epoch 4] Train loss: 0.006, acc: 98.99, time: 347.3s\n",
      "[Epoch 4] Valid loss: 0.167, acc: 84.00\n",
      "[Epoch 5] Train loss: 0.004, acc: 99.40, time: 350.4s\n",
      "[Epoch 5] Valid loss: 0.165, acc: 84.00\n",
      "[Epoch 6] Train loss: 0.003, acc: 99.48, time: 350.7s\n",
      "[Epoch 6] Valid loss: 0.144, acc: 87.00\n",
      "[Epoch 7] Train loss: 0.001, acc: 99.81, time: 349.5s\n",
      "[Epoch 7] Valid loss: 0.181, acc: 85.71\n",
      "[Epoch 8] Train loss: 0.001, acc: 99.89, time: 353.4s\n",
      "[Epoch 8] Valid loss: 0.189, acc: 85.00\n",
      "[Epoch 9] Train loss: 0.001, acc: 99.89, time: 350.9s\n",
      "[Epoch 9] Valid loss: 0.193, acc: 84.43\n",
      "Training finish! Best validation accuracy: 87.00\n",
      "\n",
      "Start trining 1th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.130, acc: 59.15, time: 351.2s\n",
      "[Epoch 0] Valid loss: 0.101, acc: 75.86\n",
      "[Epoch 1] Train loss: 0.081, acc: 82.18, time: 349.3s\n",
      "[Epoch 1] Valid loss: 0.070, acc: 84.71\n",
      "[Epoch 2] Train loss: 0.031, acc: 94.22, time: 350.0s\n",
      "[Epoch 2] Valid loss: 0.126, acc: 84.71\n",
      "[Epoch 3] Train loss: 0.013, acc: 97.93, time: 354.0s\n",
      "[Epoch 3] Valid loss: 0.132, acc: 84.29\n",
      "[Epoch 4] Train loss: 0.007, acc: 99.07, time: 353.1s\n",
      "[Epoch 4] Valid loss: 0.152, acc: 84.57\n",
      "[Epoch 5] Train loss: 0.002, acc: 99.54, time: 353.0s\n",
      "[Epoch 5] Valid loss: 0.164, acc: 84.71\n",
      "[Epoch 6] Train loss: 0.003, acc: 99.54, time: 352.5s\n",
      "[Epoch 6] Valid loss: 0.150, acc: 86.14\n",
      "[Epoch 7] Train loss: 0.002, acc: 99.73, time: 351.3s\n",
      "[Epoch 7] Valid loss: 0.168, acc: 85.71\n",
      "[Epoch 8] Train loss: 0.001, acc: 99.97, time: 354.4s\n",
      "[Epoch 8] Valid loss: 0.179, acc: 85.29\n",
      "[Epoch 9] Train loss: 0.001, acc: 99.95, time: 353.6s\n",
      "[Epoch 9] Valid loss: 0.179, acc: 85.71\n",
      "Training finish! Best validation accuracy: 86.14\n",
      "\n",
      "Start trining 2th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.140, acc: 50.97, time: 353.7s\n",
      "[Epoch 0] Valid loss: 0.134, acc: 59.14\n",
      "[Epoch 1] Train loss: 0.104, acc: 73.29, time: 353.2s\n",
      "[Epoch 1] Valid loss: 0.083, acc: 81.14\n",
      "3170/3485\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.006, acc: 99.13, time: 352.3s\n",
      "[Epoch 4] Valid loss: 0.148, acc: 83.00\n",
      "[Epoch 5] Train loss: 0.004, acc: 99.29, time: 354.9s\n",
      "[Epoch 5] Valid loss: 0.143, acc: 83.43\n",
      "[Epoch 6] Train loss: 0.003, acc: 99.56, time: 353.8s\n",
      "[Epoch 6] Valid loss: 0.193, acc: 82.86\n",
      "[Epoch 7] Train loss: 0.000, acc: 99.95, time: 356.7s\n",
      "[Epoch 7] Valid loss: 0.202, acc: 84.71\n",
      "425/425\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 0.002, acc: 99.70, time: 353.3s\n",
      "[Epoch 5] Valid loss: 0.179, acc: 85.00\n",
      "[Epoch 6] Train loss: 0.002, acc: 99.59, time: 353.9s\n",
      "[Epoch 6] Valid loss: 0.188, acc: 84.86\n",
      "[Epoch 7] Train loss: 0.001, acc: 99.84, time: 355.2s\n",
      "[Epoch 7] Valid loss: 0.184, acc: 85.00\n",
      "[Epoch 8] Train loss: 0.001, acc: 99.92, time: 355.2s\n",
      "[Epoch 8] Valid loss: 0.184, acc: 85.86\n",
      "3584/3585\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 0.035, acc: 94.00, time: 354.7s\n",
      "[Epoch 2] Valid loss: 0.100, acc: 82.00\n",
      "[Epoch 3] Train loss: 0.012, acc: 98.25, time: 357.2s\n",
      "[Epoch 3] Valid loss: 0.128, acc: 83.57\n",
      "[Epoch 4] Train loss: 0.007, acc: 98.80, time: 354.8s\n",
      "[Epoch 4] Valid loss: 0.150, acc: 82.57\n",
      "[Epoch 5] Train loss: 0.005, acc: 98.99, time: 356.3s\n",
      "[Epoch 5] Valid loss: 0.187, acc: 81.43\n",
      "1705/1705\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 0.002, acc: 99.75, time: 358.5s\n",
      "[Epoch 7] Valid loss: 0.181, acc: 83.57\n",
      "[Epoch 8] Train loss: 0.001, acc: 99.92, time: 358.0s\n",
      "[Epoch 8] Valid loss: 0.193, acc: 82.57\n",
      "[Epoch 9] Train loss: 0.000, acc: 99.97, time: 356.7s\n",
      "[Epoch 9] Valid loss: 0.209, acc: 81.86\n",
      "Training finish! Best validation accuracy: 83.57\n",
      "\n",
      "Start trining 5th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.140, acc: 51.24, time: 357.3s\n",
      "[Epoch 0] Valid loss: 0.123, acc: 68.14\n",
      "2083/2690\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.001, acc: 99.95, time: 359.1s\n",
      "[Epoch 9] Valid loss: 0.218, acc: 84.43\n",
      "Training finish! Best validation accuracy: 84.86\n",
      "\n",
      "Start trining 6th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.137, acc: 53.12, time: 359.3s\n",
      "[Epoch 0] Valid loss: 0.098, acc: 76.57\n",
      "[Epoch 1] Train loss: 0.086, acc: 81.17, time: 354.8s\n",
      "[Epoch 1] Valid loss: 0.086, acc: 81.86\n",
      "[Epoch 2] Train loss: 0.033, acc: 94.00, time: 358.5s\n",
      "[Epoch 2] Valid loss: 0.113, acc: 81.14\n",
      "2626/2690\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 0.002, acc: 99.86, time: 356.8s\n",
      "[Epoch 6] Valid loss: 0.181, acc: 84.57\n",
      "[Epoch 7] Train loss: 0.001, acc: 99.89, time: 358.8s\n",
      "[Epoch 7] Valid loss: 0.199, acc: 84.57\n",
      "[Epoch 8] Train loss: 0.000, acc: 99.95, time: 357.7s\n",
      "[Epoch 8] Valid loss: 0.205, acc: 85.29\n",
      "[Epoch 9] Train loss: 0.000, acc: 100.00, time: 355.7s\n",
      "[Epoch 9] Valid loss: 0.209, acc: 85.43\n",
      "Training finish! Best validation accuracy: 85.43\n",
      "\n",
      "Start trining 7th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/780\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 0.085, acc: 80.90, time: 355.3s\n",
      "[Epoch 1] Valid loss: 0.097, acc: 81.00\n",
      "[Epoch 2] Train loss: 0.037, acc: 93.70, time: 356.8s\n",
      "[Epoch 2] Valid loss: 0.088, acc: 84.57\n",
      "[Epoch 3] Train loss: 0.013, acc: 98.25, time: 355.4s\n",
      "[Epoch 3] Valid loss: 0.115, acc: 84.57\n",
      "[Epoch 4] Train loss: 0.009, acc: 98.64, time: 357.4s\n",
      "[Epoch 4] Valid loss: 0.121, acc: 85.00\n",
      "2834/2855\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 0.015, acc: 97.44, time: 357.1s\n",
      "[Epoch 3] Valid loss: 0.130, acc: 83.71\n",
      "[Epoch 4] Train loss: 0.006, acc: 99.02, time: 356.7s\n",
      "[Epoch 4] Valid loss: 0.135, acc: 84.29\n",
      "[Epoch 5] Train loss: 0.003, acc: 99.48, time: 356.5s\n",
      "[Epoch 5] Valid loss: 0.177, acc: 84.00\n",
      "[Epoch 6] Train loss: 0.001, acc: 99.95, time: 359.4s\n",
      "[Epoch 6] Valid loss: 0.232, acc: 84.00\n",
      "3238/3245\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.001, acc: 99.86, time: 361.4s\n",
      "[Epoch 9] Valid loss: 0.224, acc: 83.71\n",
      "Training finish! Best validation accuracy: 84.29\n",
      "\n",
      "Start trining 9th model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.141, acc: 51.65, time: 359.3s\n",
      "[Epoch 0] Valid loss: 0.153, acc: 46.57\n",
      "[Epoch 1] Train loss: 0.104, acc: 74.05, time: 357.5s\n",
      "[Epoch 1] Valid loss: 0.088, acc: 79.43\n",
      "[Epoch 2] Train loss: 0.048, acc: 90.86, time: 356.8s\n",
      "[Epoch 2] Valid loss: 0.084, acc: 82.71\n",
      "[Epoch 3] Train loss: 0.017, acc: 97.57, time: 357.0s\n",
      "[Epoch 3] Valid loss: 0.150, acc: 79.57\n",
      "25/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.007, acc: 98.83, time: 360.3s\n",
      "[Epoch 4] Valid loss: 0.162, acc: 83.14\n",
      "[Epoch 5] Train loss: 0.004, acc: 99.32, time: 360.5s\n",
      "[Epoch 5] Valid loss: 0.185, acc: 83.57\n",
      "[Epoch 6] Train loss: 0.004, acc: 99.51, time: 360.3s\n",
      "[Epoch 6] Valid loss: 0.161, acc: 84.29\n",
      "[Epoch 7] Train loss: 0.003, acc: 99.62, time: 360.3s\n",
      "[Epoch 7] Valid loss: 0.164, acc: 83.86\n",
      "1589/1590\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning networks (10 repeat)\n",
    "lr = 8e-6\n",
    "num_epochs = 10\n",
    "save_dir = './results_qa'\n",
    "\n",
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "config.nclass = 2\n",
    "for i in range(10):\n",
    "    train(i, num_epochs, lr, train_loader, val_loader, config=config, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 0th model (acc: 87.00)\n",
      "Load 1th model (acc: 86.14)\n",
      "Load 2th model (acc: 85.43)\n",
      "Load 3th model (acc: 85.86)\n",
      "Load 4th model (acc: 83.57)\n",
      "Load 5th model (acc: 84.86)\n",
      "Load 6th model (acc: 85.43)\n",
      "Load 7th model (acc: 85.43)\n",
      "Load 8th model (acc: 84.29)\n",
      "Load 9th model (acc: 84.29)\n",
      "\n",
      "Ensemble accuracy: 88.14\n"
     ]
    }
   ],
   "source": [
    "answer = torch.tensor(val_dataset.labels)\n",
    "save_dir = './results_qa'\n",
    "\n",
    "validate_ensemble(save_dir, model_name, val_loader, answer, idx_max=10, acc_threshold=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 동형이의어 구별 (BoolQ)\n",
    "\n",
    "- 전체적인 코드의 뼈대 / 하이퍼 파라미터는 같은 조의 BoolQ의 코드를 사용함\n",
    "- 사용모델 : KLUE-roberta-large\n",
    "- Epoch: 10\n",
    "- warm up: 10% training step\n",
    "- Learning rate: 8e-6\n",
    "- Batch size: 4 (colab에서 4를 넘어가면 memory 초과)\n",
    "- Classifier: 1 layer linear model ( layer/activaation을 추가해도 효과 미미)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "xhTg1v1u6Mhj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3824,
     "status": "ok",
     "timestamp": 1638773692906,
     "user": {
      "displayName": "­안형서 / 학생 / 전기·정보공학부",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2m5GEkg1AKOub_GkVdF2Ujkorj8KoGoebIxj9=s64",
      "userId": "10372927598583850844"
     },
     "user_tz": -540
    },
    "id": "xhTg1v1u6Mhj",
    "outputId": "fe0be487-0f03-4e94-e16a-f3b5b72f1545"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers  #if colab !!!!!!!!\n",
    "#cur_dir=\"./drive/MyDrive/NLP/final/\"   #if colab !!!!!!!!\n",
    "\n",
    "#cur_dir=\"../\" # if local!!!!!!!!!!!!!!1\n",
    "cur_dir=\"./\" # if local!!!!!!!!!!!!!!1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510a000b",
   "metadata": {
    "id": "510a000b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import pandas\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaModel, RobertaConfig ,RobertaPreTrainedModel\n",
    "from transformers import AdamW\n",
    "import time\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "transformers.logging.set_verbosity(40) # Turn off warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "-42TlySu6ktN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59284,
     "status": "ok",
     "timestamp": 1638773772090,
     "user": {
      "displayName": "­안형서 / 학생 / 전기·정보공학부",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2m5GEkg1AKOub_GkVdF2Ujkorj8KoGoebIxj9=s64",
      "userId": "10372927598583850844"
     },
     "user_tz": -540
    },
    "id": "-42TlySu6ktN",
    "outputId": "3dc40d9d-51ac-4573-be85-02b750951b27"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive') #if colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1389aa39",
   "metadata": {
    "id": "1389aa39"
   },
   "outputs": [],
   "source": [
    "ensen_num=6\n",
    "\n",
    "save_dir = cur_dir+'result_wic'\n",
    "\n",
    "if not os.path.exists( save_dir):\n",
    "    os.makedirs(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uILgm5rRN1fZ",
   "metadata": {
    "id": "uILgm5rRN1fZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nseed=5555\\n\\ntorch.manual_seed(seed)\\ntorch.backends.cudnn.deterministic = True\\ntorch.backends.cudnn.benchmark = False\\nnp.random.seed(seed)\\nrandom.seed(seed)\\nos.environ['PYTHONHASHSEED'] = str(seed)\\nif torch.cuda.is_available():\\n    torch.cuda.manual_seed(seed)\\n    torch.cuda.manual_seed_all(seed)\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing randomness\n",
    "\n",
    "\"\"\"\n",
    "seed=5555\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00240661",
   "metadata": {
    "id": "00240661"
   },
   "source": [
    "### Load data\n",
    "- Train, Dev 데이터가 base_path에 들어있어야 합니다. (default: './data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5uU0kKI2dr0G",
   "metadata": {
    "id": "5uU0kKI2dr0G"
   },
   "source": [
    "# 데이터셋 만들기\n",
    " - 그 토큰이 명확하게 쪼개져야 하기때문에 앞뒤로 special token을 삽입해 줌으로써 그 토큰이 앞이나 뒤에서 다른 글자와 합쳐지지 않고 명확하게 쪼개지게 해준다.\n",
    " - 또한, 모델의 끝부분에서 그 토큰의 마지막 layer의 출력 값을 입력으로 쓰고 나머지 토큰들의 결과는 안쓰니, 그 부분만 1, 나머지는 0 인 mask가 필요하다. 이를 위해 만든 speical token을 이용하여 mask를 만들어준다.\n",
    " - 처음 시도는 torch.roll(shift 연산)을 사용해 동형이의어의 끝 부분을 한칸 앞으로 당긴곳을 1로 만들었지만, 이 경우, 동형이의어가 2개이상의 토큰으로 나뉜경우 완전히 커버할 수 없음.\n",
    " - 따라서 torch.cumsum(누적합 연산)을 사용하여 동형이의어의 시작/끝부분 사이를 전부 1이되도록 만들어줌\n",
    " - 1인 곳의 embedding vector는 전부 합함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd6faee",
   "metadata": {
    "id": "fdd6faee"
   },
   "outputs": [],
   "source": [
    "def load_data(path, tokenizer):\n",
    "    dataset = pandas.read_csv(path,\n",
    "                              delimiter='\\t',\n",
    "                              names=['ID', 'Target', 'text1', 'text2','answer','s_s1','e_s1','s_s2','e_s2'],\n",
    "                              header=0)\n",
    "\n",
    "    dataset_text1=[]\n",
    "    dataset_text2=[]\n",
    "\n",
    "    for i,text1 in enumerate(dataset['text1']):\n",
    "        text= text1[:dataset[\"s_s1\"][i]]+\"[WORD1S]\"+text1[dataset[\"s_s1\"][i]:dataset[\"e_s1\"][i]]+\"[WORD1E]\"+text1[dataset[\"e_s1\"][i]:]\n",
    "        dataset_text1.append(text)\n",
    "\n",
    "    for i,text2 in enumerate(dataset['text2']):\n",
    "        text= text2[:dataset[\"s_s2\"][i]]+\"[WORD2S]\"+text2[dataset[\"s_s2\"][i]:dataset[\"e_s2\"][i]]+\"[WORD2E]\"+text2[dataset[\"e_s2\"][i]:]\n",
    "        dataset_text2.append(text)\n",
    "    \n",
    "    word1s_tok_idx=tokenizer.encode(\"[WORD1S]\")[1]\n",
    "    word1e_tok_idx=tokenizer.encode(\"[WORD1E]\")[1]\n",
    "    word2s_tok_idx=tokenizer.encode(\"[WORD2S]\")[1]\n",
    "    word2e_tok_idx=tokenizer.encode(\"[WORD2E]\")[1]\n",
    "\n",
    "    tokenized = tokenizer(dataset_text1,\n",
    "                          dataset_text2,\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          return_tensors=\"pt\")\n",
    "    dataset['label'] = torch.tensor(dataset['answer'],dtype=int)\n",
    "\n",
    "    #print(tokenized[\"input_ids\"][0])\n",
    "\n",
    "    #word_mask1=  torch.roll((tokenized[\"input_ids\"]==word1e_tok_idx),-1,dims=1).int()\n",
    "    #word_mask2=  torch.roll((tokenized[\"input_ids\"]==word2e_tok_idx),-1,dims=1).int()\n",
    "    \n",
    "    word_mask1 = torch.roll((tokenized[\"input_ids\"]==word1s_tok_idx),1,dims=1).int()\n",
    "    word_mask1 = word_mask1 - (tokenized[\"input_ids\"]==word1e_tok_idx).int()\n",
    "    word_mask1 = torch.cumsum(word_mask1,dim=1)\n",
    "\n",
    "    word_mask2 = torch.roll((tokenized[\"input_ids\"]==word2s_tok_idx),1,dims=1).int()\n",
    "    word_mask2 = word_mask2 - (tokenized[\"input_ids\"]==word2e_tok_idx).int()\n",
    "    word_mask2 = torch.cumsum(word_mask2,dim=1)\n",
    "    \n",
    "\n",
    "    #print(word_mask[0][0])\n",
    "    #print(word_mask[1][0])\n",
    "\n",
    "    #print( torch.sum(torch.roll((tokenized[\"input_ids\"]==worde_tok_idx),-2,dims=1).int()-(tokenized[\"input_ids\"]==words_tok_idx).int()) )\n",
    "\n",
    "    return dataset, tokenized, word_mask1, word_mask2\n",
    "\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset, word_mask1, word_mask2, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.word_mask1= word_mask1\n",
    "        self.word_mask2= word_mask2\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item[\"word_mask1\"]=self.word_mask1[idx]\n",
    "        item[\"word_mask2\"]=self.word_mask2[idx]\n",
    "        label = self.labels[idx]\n",
    "        return item, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f0d9c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182,
     "referenced_widgets": [
      "e7b9ae64a8324fe8b5df66cd3db84ad4",
      "51f932bab4164a859354fa8f46fef300",
      "d1d5c221ae284734a164d90db8031b1b",
      "88749eaaeb914c9297d2b995b84a7fc3",
      "d32e30db2699443181706587ed26cbc0",
      "7551cef0ca3a4bab9b61dcfad0d0810a",
      "28f4e4c0adc8487290e2ad42ab13891e",
      "ba21e8c4ca9b41299aad91955e82da90",
      "299199ce14d34dc5851b6506d5255da5",
      "5daaf6d1b9564fdfb7d21d937d68cd49",
      "801423f0aceb49fc84cb185c3c2bd4e2",
      "1f9a2bf5a679473a82b6410477b16d8e",
      "5d7b6054f71d41f3bde0644a207bef39",
      "121cdf326fe6489b8be00001d51580c8",
      "f9332da451da4707a848ac81993fa273",
      "c25341d753d043eda43dd312c5b58b08",
      "02884f3d376940358195348bd5d22dae",
      "0c90a07cd64041cdbb6bdd12eb345871",
      "db14c4a3323742cf89e9fda3b52aaf91",
      "7fab521885d646758a45aa0b18c12faf",
      "50c5c827ba3044f7aa74fa50d24e7d24",
      "e499f866014f4f8397cc68dd4f47757e",
      "160060d1a1b64744b27da5d1cd4ecd96",
      "39f928c975b34ac6af4edf9219bb3c25",
      "10d16041d754440f9dce88b1f5f6e8a4",
      "069900efe9ef4b73a255fdc9cba4d279",
      "96e1b78a086747aea733e929474fe550",
      "9c663018eed044f1b79fbb91bb88239f",
      "8c1224ac8a7e4e88a98738b53599283d",
      "8055911f9b004abe8b93564a3d829178",
      "35c48c7db7814fe18f9702177331c668",
      "0efe47bbffc34cb39a79498da8557833",
      "955ffaa60f11472687233a21f08ea868",
      "58755bf65d614a8f804465973b12d371",
      "f3bc2d9c070e4eca948ee83315193a23",
      "3477d4415e52479c9dcfb0de622216ec",
      "78af815adc684f27a56fb0f8ed8e9d22",
      "bd285c5ba3464c979f1fb38a4bfb8507",
      "9fda5868a87845da89a1bf9bd1b8dab9",
      "81f83496d5464376aadd9df4c9db901c",
      "c9f0ac9b91484d6b87787c36e6c5eafd",
      "29b4a0e0d57b45f7a917a3020f3c828f",
      "a151991f36064bf39cef99fe20476c86",
      "b2f82c054ebf454884ecfc9d3ed5e4eb"
     ]
    },
    "executionInfo": {
     "elapsed": 1923,
     "status": "ok",
     "timestamp": 1638773798671,
     "user": {
      "displayName": "­안형서 / 학생 / 전기·정보공학부",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2m5GEkg1AKOub_GkVdF2Ujkorj8KoGoebIxj9=s64",
      "userId": "10372927598583850844"
     },
     "user_tz": -540
    },
    "id": "27f0d9c2",
    "outputId": "8a83f451-4d42-4f8d-d39b-dae51bf56c1f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad837f2823884f43b96e84dee65005a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177fd71470e540caad7eb61845cdd16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a36908c500446a9f74db9d5d6ee758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/734k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89a871a1ee84141bcd9bf4c20c31716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 4 new special token\n",
      "32004\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_type = \"Roberta\"\n",
    "size = 'large'\n",
    "model_name = f\"klue/roberta-{size}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "new_special_tokens=[\"[WORD1S]\",\"[WORD1E]\",\"[WORD2S]\",\"[WORD2E]\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\":new_special_tokens})\n",
    "print(f\"added {len(new_special_tokens)} new special token\")\n",
    "\n",
    "print(len(tokenizer))\n",
    "\n",
    "# add special token - 동형이의어를 문장과 구별하기 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4082ecb",
   "metadata": {
    "id": "d4082ecb"
   },
   "outputs": [],
   "source": [
    "\n",
    "base_path = cur_dir+'data'\n",
    "\n",
    "train_dataset, train_tokenized, train_word_mask1, train_word_mask2 = load_data(os.path.join(base_path, 'NIKL_SKT_WiC_Train.tsv'),\n",
    "                                           tokenizer)\n",
    "val_dataset, val_tokenized, val_word_mask1, val_word_mask2 = load_data(os.path.join(base_path, 'NIKL_SKT_WiC_Dev.tsv'), tokenizer)\n",
    "\n",
    "train_dataset = TensorDataset(train_tokenized, train_word_mask1, train_word_mask2, train_dataset['label'])\n",
    "val_dataset = TensorDataset(val_tokenized, val_word_mask1, val_word_mask2, val_dataset['label'])\n",
    "\n",
    "if size == 'base':\n",
    "    batch_size = 16\n",
    "else:\n",
    "    batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38fd4618",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1638773803864,
     "user": {
      "displayName": "­안형서 / 학생 / 전기·정보공학부",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2m5GEkg1AKOub_GkVdF2Ujkorj8KoGoebIxj9=s64",
      "userId": "10372927598583850844"
     },
     "user_tz": -540
    },
    "id": "38fd4618",
    "outputId": "b3bf4eb9-b5fc-45a6-9c9c-b61f3b02b25e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 그의 죽음은 타살로 [WORD1S] 단정 [WORD1E] 이 되었다. [SEP] [WORD2S] 단정 [WORD2E] 이 된 교실은 정돈되어 있다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 예시\n",
    "tokenizer.decode(train_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qpmQrODfJddO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1638773803865,
     "user": {
      "displayName": "­안형서 / 학생 / 전기·정보공학부",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2m5GEkg1AKOub_GkVdF2Ujkorj8KoGoebIxj9=s64",
      "userId": "10372927598583850844"
     },
     "user_tz": -540
    },
    "id": "qpmQrODfJddO",
    "outputId": "fefd6b77-9257-4bd3-c520-6c4476ae376f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 예시2, 동형이의어 마스크\n",
    "train_word_mask1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1965e2",
   "metadata": {
    "id": "9d1965e2"
   },
   "source": [
    "### Load pretrained model\n",
    "- 모델은 KLUE-RoBERTa를 사용하였습니다. (https://github.com/KLUE-benchmark/KLUE) \n",
    "- 각 문장의 동형이의어 마스크를 통과한 결과를 합해줌으로써 각 문장의 동형이의어를 대표하는 hid_dim크기의 벡터를 2개 만듭니다.\n",
    "- 2개의 벡터를 concat하여 classifier에 넣고 구분하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hHgJ7tFtfhrB",
   "metadata": {
    "id": "hHgJ7tFtfhrB"
   },
   "source": [
    "### 모델 수정 시도\n",
    "- 1. **1 layer classifier** (92.54%)\n",
    "- 2. 2 layer classifier (92.11%)\n",
    "- 3. 2 layer classifier with relu (91.25%)\n",
    "- 4. 2 layer classifier with tanh (91.42%)\n",
    "- layer의 복잡성을 올려도, 효과는 미미한데 학습 속도는 느려짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b89d10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597,
     "referenced_widgets": [
      "19840fe61a604114beb024b311d4a285",
      "d12e62a316b44fd58193b59415eefb81",
      "6429cfa60438473681a12c9a934037d5",
      "1e28ee7a61424f42b5ed4dffc04c7ff5",
      "969d2acf036842479cf6d8292159844d",
      "71543a3e83d24ad0afd7fa53ffeca663",
      "1288586920f44f03b9851da656c06ef7",
      "07edb72ac5834f8d8b1bedff5d2978e6",
      "7884c320d8fc4f43a856354261972511",
      "be8b1780c4ac42faa6430167603288d7",
      "26fb45034d54460ea7c0cd16ad0cc6d2"
     ]
    },
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1638773804392,
     "user": {
      "displayName": "­안형서 / 학생 / 전기·정보공학부",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2m5GEkg1AKOub_GkVdF2Ujkorj8KoGoebIxj9=s64",
      "userId": "10372927598583850844"
     },
     "user_tz": -540
    },
    "id": "d0b89d10",
    "outputId": "a1d63d5d-829e-4e11-8343-67b8f4401cf3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1f8df553f54981b2f055f0ba48c2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"nclass\": 2,\n",
      "  \"new_tok_size\": 32004,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Roberta(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, model_name):\n",
    "        super(Roberta, self).__init__(config)\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name, config=RobertaConfig.from_pretrained(model_name))\n",
    "        self.roberta.resize_token_embeddings(config.new_tok_size)\n",
    "        self.hdim = config.hidden_size\n",
    "        self.nclass = config.nclass\n",
    "        self.classifier = nn.Linear(self.hdim*2, self.nclass)\n",
    "        #self.classifier = nn.Linear(self.hdim*2, self.hdim)\n",
    "        #self.activation = nn.ReLU()\n",
    "        #self.activation = nn.Tanh()\n",
    "        #self.classifier2 = nn.Linear(self.hdim, self.nclass)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, word_mask1, word_mask2 , **kwargs):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)[0]\n",
    "        #(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        #word_mask  ( 2, batch , seq_len)\n",
    "\n",
    "        #word_mask[0] (batch, seq_len)  [[0,0,0,0,0,1,0,0,0,0],[...],..]\n",
    "        #word_mask[0].unsqueeze(2) (batch, seq_len,1) \n",
    "        \n",
    "        #torch.sum( word_mask[0].unsqueeze(2) * outputs , dim=1 )   # (batch,  hidden_size)\n",
    "        #word_mask[1].unsqueeze(2) * outputs\n",
    "        \n",
    "        h = torch.cat( [torch.sum( word_mask1.unsqueeze(2) * outputs , dim=1 ),torch.sum( word_mask2.unsqueeze(2) * outputs , dim=1 )], dim=1)\n",
    "        # (batch, hidden_size*2)\n",
    "\n",
    "\n",
    "        logits = self.classifier(h)\n",
    "\n",
    "        #h1 = self.classifier(h)\n",
    "        #h1_a = self.activation(h1)\n",
    "        #logits = self.classifier2(h1_a)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "config.nclass = 2\n",
    "config.new_tok_size = len(tokenizer)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16f6361",
   "metadata": {
    "id": "f16f6361"
   },
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    cor = 0\n",
    "    n_sample = 0\n",
    "    s = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for data, target in tqdm(train_loader):\n",
    "        item = {key: val.to(device) for key, val in data.items()}\n",
    "        target = target.to(device)\n",
    "\n",
    "        logits = model(**item)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        cor += (preds == target).sum().item()\n",
    "        n_sample += len(target)\n",
    "\n",
    "        print(f\"{cor}/{n_sample}\", end='\\r')\n",
    "\n",
    "    loss_avg = total_loss / n_sample\n",
    "    acc = cor / n_sample\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] Train loss: {loss_avg:.3f}, acc: {acc*100:.2f}, time: {time.time()-s:.1f}s\"\n",
    "    )\n",
    "    return acc\n",
    "\n",
    "\n",
    "def validate(epoch, model, val_loader, verbose=True):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    cor = 0\n",
    "    n_sample = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            item = {key: val.to(device) for key, val in data.items()}\n",
    "            target = target.to(device)\n",
    "\n",
    "            logits = model(**item)\n",
    "            loss = criterion(logits, target)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            pred_all.append(preds)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            cor += (preds == target).sum().item()\n",
    "            n_sample += len(target)\n",
    "\n",
    "    loss_avg = total_loss / n_sample\n",
    "    acc = cor / n_sample\n",
    "    pred_all = torch.cat(pred_all)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[Epoch {epoch}] Valid loss: {loss_avg:.3f}, acc: {acc*100:.2f}\")\n",
    "    return acc, pred_all\n",
    "\n",
    "\n",
    "def train(idx, num_epochs, lr, train_loader, val_loader, tokenizer):\n",
    "    print(f\"Start trining {idx}th model\")\n",
    "    model = Roberta(config, model_name).to(device)\n",
    "\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = transformers.get_scheduler(\"linear\",\n",
    "                                           optimizer=optimizer,\n",
    "                                           num_warmup_steps=num_epochs * len(train_loader) // 10,\n",
    "                                           num_training_steps=num_epochs * len(train_loader))\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc = train_epoch(epoch, model, train_loader, optimizer, scheduler)\n",
    "        val_acc, _ = validate(epoch, model, val_loader)\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "            if not os.path.exists( os.path.join(save_dir, f'{idx}') ):\n",
    "                os.makedirs( os.path.join(save_dir, f'{idx}') )\n",
    "\n",
    "            model_to_save.save_pretrained(os.path.join(save_dir, f'{idx}'))\n",
    "            \n",
    "    print(f\"Training finish! Best validation accuracy: {best_acc*100:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca3c1fe2",
   "metadata": {
    "id": "ca3c1fe2"
   },
   "outputs": [],
   "source": [
    "lr = 8e-6\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c889cc9",
   "metadata": {
    "id": "3c889cc9"
   },
   "source": [
    "##  실행 결과\n",
    " - 각 모델은 대략 91%~92% 정도의 결과를 보임\n",
    " - 6번 실행. 이후 앙상블 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17295cee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510,
     "referenced_widgets": [
      "d3b1bdf0f6e94ba5a7265a60b79c1add",
      "19a03d7c4d0b497785d39b17f532c694",
      "7147b5f24c364c84b8a097d7a6e0bfcb",
      "5b3af215fef1467189bcf3c162235cfa",
      "53f4362119b64227b539544e7ca049ec",
      "f5cba34c6f0a4fd7b18b17d958079944",
      "28dae571960e4b0c990cca0b8fdc0f04",
      "d3efe9ef42b041ffb3ea83d002ea0da5",
      "66651868655747668e3ee2633d0f49f8",
      "dab704bdac3942bc8c295477a4315025",
      "e1e6cce6a138421fb878be27dea963e5",
      "d0cf925f522344ad8cf9dc9e553a70e2",
      "0273d910839844e5aa8f3b030313fe4e",
      "241f458aade249c7adfd6bcd8868ec8d",
      "c57deec5f3d44beabdb057c8f752fcac",
      "1c06cee1092e460e96cb1c147fc02368",
      "7eb8b53ba5b4424691e564e555a0a40c",
      "0b67432930374d21b224b5c9feea1db3",
      "aba079fe3e4b400bbd3e11ea0d76c82a",
      "6d5143beca1b454c86db95ae0d68d260",
      "e26271bca156464187176908cbab3bf6",
      "c1db2744cd474b39bacbaf530ecd9c7f",
      "7ca3c68aff554e9f8b0a2c726c454232",
      "b9b24d3425364d99a154aa72bc778a6f",
      "64f4f54b340840649c2044f16b6e5792",
      "e4a93db66e6b4ea98cec35517050eb60",
      "44a53774cf7c43ef83ce85303bc57bc4",
      "9af0b5d4478d485cb2645135e8e36e8b",
      "1ae8b58334ab4845ad0a305b9bc40e7c",
      "ba611529b4e14f32ac04bdc51a08d6f9",
      "2c8850e42b124e20be35f231d2afb0fa",
      "79ab8c8b6b1240a6980e5f9e385b7977",
      "ba6a5bd87dae4461bdba265cdfced139",
      "e577cbc799874e3bb59b8bcabc13f332",
      "b9ec8a0e0259454ab2f15081604c0234",
      "7a348cb05a4745c3a87f4c9f17f9cae7",
      "f73f92181ec34432a72c68238c2e7ecf",
      "52b036d6d2e8441b939f87f659854669",
      "14d612f32efe408fbf245aea1dbf5dcb",
      "de2ba0cc7b0641d5bc1de689b4386f69",
      "f613429862bc4c068f822f1650341495",
      "d5187f8a77b442e69fc10607531f54da",
      "4643c81468c4402aa9942756abf8c255",
      "35ce95bf746c4b839b11ce8a7e0d6288",
      "52f8a8a874374bacad31ec7622255fef",
      "5e23cc8c4c664712b30a97f7a0e30c86",
      "267781abe5d84ebba9c2140c1fa25fd2",
      "b9dbd52d5b5f4a1aab16e453f786158e",
      "035ebf00ccb1451cafebd1b667a031fd",
      "533274a1bf094fc3932726ba20e96904",
      "8003e932b98141ffbc4dacd3cb31d1a9",
      "c7eee685d6b0494d95256a633ea92a99",
      "3224c4d939444fee89884088c584d26a",
      "cf05cd3748ae4fe8b4b8c42277592f64",
      "7cb7662881ef4c9f80e14ef88e4b71d7",
      "b5171a1a6c904f9d92e3361982c6c35d",
      "935e73e96b1a44aebab2a282685d734e",
      "d0e9707bdba2415bacd070939de7da7c",
      "d22ac7e4aca04cd4b529b62786ef40a9",
      "fd93747567734f7a897a2ed48c29c0c4",
      "0b7f14ea012d4ea7a61e51d984e89f5a",
      "99df3a6e1477474c8c5ada7c433fc454",
      "57eae92a0ad84ae68adf7bbfec56eba6",
      "3a8a58f207e748418921cc1dcfce6b9b",
      "3c0521a988364faeb7a8aa21fe5194d0",
      "3251f61e5dc1421788b3bd7b00549f4c",
      "e7d1dc50e5254255a12293c618a5bdf9",
      "254cf2aa3cea40a680b3d0c6452c1798",
      "396928f391b4403f8a17ab0d0215ac52",
      "27c074c9345046c08b43db36a0e6ce94",
      "94e2bd93a26c4fc69e17e2c7f9b54639",
      "59405b18b4f74710bc48919bb43ef000",
      "750730f99f8541f692029584273d36b3",
      "1fb23fa10ef44967a63c2bed8361879c",
      "c73c339fa3e84ce39fb0d9e920dd404c",
      "842c57f9cdba48d5853c346c9363cc72",
      "657399d477d741479d2d5c06a95228c6",
      "f2a93aec43214930862fb34b102142c2",
      "82ac3bc75c584fe6964052fe70b1fc56",
      "3a939d72e5f84cf98796046c663ed093",
      "1a24799f318e477e93747d545260cc48",
      "8856be9a74014a2bb86a2f96d7b64df8",
      "187443649b7e440d9de529c5a40b4ea3",
      "e56a8095076e4812ba602f97b3102753",
      "67d82c14f5cf4cab802bbae68e7662e6",
      "d0f3e6e190764ab2a5b82633bc1d0424",
      "09832ebc74614364ac1d38b2639d71ce",
      "dc28547cf16847549318685291badcc4"
     ]
    },
    "id": "17295cee",
    "outputId": "fa0092bb-957b-4803-8c62-5928f270a298",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trining 0th model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89fd550b0aa40c9abacc9eca6cca224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2874c08b6b7a4babbe03aa2f21c9ef9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.127, acc: 73.70, time: 429.8s\n",
      "[Epoch 0] Valid loss: 0.084, acc: 87.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61979c5e38a46cbac2be9172102cd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 0.064, acc: 90.49, time: 429.7s\n",
      "[Epoch 1] Valid loss: 0.071, acc: 88.16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc713e012b624765999b05b7f4029ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 0.026, acc: 96.32, time: 429.9s\n",
      "[Epoch 2] Valid loss: 0.073, acc: 91.17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54b6be1277146968e8482ab10a12566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 0.012, acc: 98.46, time: 430.1s\n",
      "[Epoch 3] Valid loss: 0.093, acc: 90.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303ff099993b4dc08125ba67cc4e7932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.007, acc: 99.01, time: 429.5s\n",
      "[Epoch 4] Valid loss: 0.090, acc: 91.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd67f7141e73402bb3195af744b89c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 0.005, acc: 99.48, time: 432.8s\n",
      "[Epoch 5] Valid loss: 0.106, acc: 90.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18dd2c480fb6486eb962dc1496d33be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 0.002, acc: 99.81, time: 432.7s\n",
      "[Epoch 6] Valid loss: 0.117, acc: 91.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d79871230ad4ac49954124a993235e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 0.001, acc: 99.91, time: 432.9s\n",
      "[Epoch 7] Valid loss: 0.133, acc: 92.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be21a53ecdd4a709bf2bd23dc682427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 0.000, acc: 99.99, time: 433.2s\n",
      "[Epoch 8] Valid loss: 0.148, acc: 92.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1aefeab5744998867341a81992e531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.001, acc: 99.94, time: 432.8s\n",
      "[Epoch 9] Valid loss: 0.142, acc: 92.28\n",
      "Training finish! Best validation accuracy: 92.28\n",
      "\n",
      "Start trining 1th model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea14b1cd102b42ec8678f14f52a09376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.127, acc: 72.66, time: 432.9s\n",
      "[Epoch 0] Valid loss: 0.085, acc: 85.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cec1e916fc494bb3b7a307eaa882ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 0.057, acc: 91.29, time: 433.7s\n",
      "[Epoch 1] Valid loss: 0.076, acc: 88.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b564110e02aa4ef1b3ff35f24784cd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 0.021, acc: 97.11, time: 433.5s\n",
      "[Epoch 2] Valid loss: 0.069, acc: 90.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3707cb2510427b97addb3f3b5dea14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 0.011, acc: 98.66, time: 432.9s\n",
      "[Epoch 3] Valid loss: 0.103, acc: 88.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5984a260804193b0e34fad6e9de82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.006, acc: 99.37, time: 433.1s\n",
      "[Epoch 4] Valid loss: 0.104, acc: 90.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed1382514a844908ddc2d0a67a9e379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 0.004, acc: 99.57, time: 433.3s\n",
      "[Epoch 5] Valid loss: 0.089, acc: 90.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6219f36b65794bd2bd78d82a79c33b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 0.002, acc: 99.81, time: 432.8s\n",
      "[Epoch 6] Valid loss: 0.101, acc: 92.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70d34089a524be290989634a5153ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 0.000, acc: 99.95, time: 433.1s\n",
      "[Epoch 7] Valid loss: 0.130, acc: 91.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c73f74f7ff4f7a8a1e016c1d47d043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 0.001, acc: 99.94, time: 433.1s\n",
      "[Epoch 8] Valid loss: 0.140, acc: 91.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b217e14e7e7e4e72a26badedce62fe5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.000, acc: 99.99, time: 433.3s\n",
      "[Epoch 9] Valid loss: 0.130, acc: 92.20\n",
      "Training finish! Best validation accuracy: 92.54\n",
      "\n",
      "Start trining 2th model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fee06c73879420bbd828ad53751657f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.130, acc: 72.73, time: 433.2s\n",
      "[Epoch 0] Valid loss: 0.091, acc: 84.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c80ed1ccd75494c9829475ec2d8fe47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 0.056, acc: 91.62, time: 433.4s\n",
      "[Epoch 1] Valid loss: 0.071, acc: 88.34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ead3a2aa4804807b4010288d45408a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 0.022, acc: 97.02, time: 433.3s\n",
      "[Epoch 2] Valid loss: 0.079, acc: 90.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7f0c68bc384db99e9265b41cc1b88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 0.010, acc: 98.50, time: 433.1s\n",
      "[Epoch 3] Valid loss: 0.096, acc: 90.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e845d19eac43dabba191d1b4bb7109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.006, acc: 99.16, time: 433.2s\n",
      "[Epoch 4] Valid loss: 0.095, acc: 91.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49118cc62994f89a5f3d12562730e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 0.003, acc: 99.64, time: 434.0s\n",
      "[Epoch 5] Valid loss: 0.121, acc: 90.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2119d07f727940a398970b51cc460863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 0.002, acc: 99.74, time: 433.7s\n",
      "[Epoch 6] Valid loss: 0.107, acc: 91.17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985715dc3d254924b7b870dd4706141e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 0.002, acc: 99.83, time: 433.5s\n",
      "[Epoch 7] Valid loss: 0.114, acc: 91.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5439f4817194d58b89b8e4866935b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 0.000, acc: 100.00, time: 433.7s\n",
      "[Epoch 8] Valid loss: 0.128, acc: 92.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43d28cbd14c464c9fe46ccea8ab496b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.000, acc: 99.96, time: 433.2s\n",
      "[Epoch 9] Valid loss: 0.134, acc: 91.94\n",
      "Training finish! Best validation accuracy: 92.11\n",
      "\n",
      "Start trining 3th model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f727d6fb44e47e3a43291a017147cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.126, acc: 73.52, time: 434.0s\n",
      "[Epoch 0] Valid loss: 0.076, acc: 87.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4f07e195dd4c4688e37c0106616c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 0.059, acc: 90.95, time: 433.2s\n",
      "[Epoch 1] Valid loss: 0.070, acc: 89.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e89fe2bf4884116a6296c6be41fb690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 0.023, acc: 96.77, time: 433.9s\n",
      "[Epoch 2] Valid loss: 0.076, acc: 90.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dfda0cddfa4fb38cd15d6c63efa1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 0.013, acc: 98.26, time: 433.5s\n",
      "[Epoch 3] Valid loss: 0.132, acc: 88.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad065d4c47941e99fdd9c8138791e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.007, acc: 99.04, time: 433.3s\n",
      "[Epoch 4] Valid loss: 0.072, acc: 91.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f91442d5c08493a88230d1eb4dde94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 0.003, acc: 99.65, time: 434.6s\n",
      "[Epoch 5] Valid loss: 0.127, acc: 90.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f29546d922440e895e2179ef2e4b40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 0.002, acc: 99.73, time: 433.4s\n",
      "[Epoch 6] Valid loss: 0.166, acc: 87.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8615ff6888d64417a5e21f3229d1cb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 0.001, acc: 99.86, time: 433.6s\n",
      "[Epoch 7] Valid loss: 0.122, acc: 90.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b481ee89a4cc4246ad21d46874551732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 0.001, acc: 99.92, time: 433.9s\n",
      "[Epoch 8] Valid loss: 0.118, acc: 92.20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96587f7e74c14ce188c56fbd2f811f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.000, acc: 99.94, time: 433.7s\n",
      "[Epoch 9] Valid loss: 0.122, acc: 92.11\n",
      "Training finish! Best validation accuracy: 92.20\n",
      "\n",
      "Start trining 4th model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92703d6141de4410b9892bb84c9e40e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.126, acc: 74.42, time: 432.7s\n",
      "[Epoch 0] Valid loss: 0.085, acc: 84.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b138731dcf4c00bbcd482437d7a8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 0.058, acc: 91.34, time: 433.5s\n",
      "[Epoch 1] Valid loss: 0.064, acc: 89.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb2d17f0dd5404ea2b62773d735dfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 0.022, acc: 96.97, time: 432.9s\n",
      "[Epoch 2] Valid loss: 0.062, acc: 90.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122617d25d424df9b529ce60914dc9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 0.011, acc: 98.70, time: 433.6s\n",
      "[Epoch 3] Valid loss: 0.079, acc: 91.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f891e1b0f34f1ba0e994cb59b1d175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.007, acc: 99.06, time: 433.2s\n",
      "[Epoch 4] Valid loss: 0.077, acc: 91.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139b7bb2624e4b8d95aadf3714e2594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 0.003, acc: 99.54, time: 433.6s\n",
      "[Epoch 5] Valid loss: 0.106, acc: 91.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d088cd2256458d887e0b0f972f2d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 0.002, acc: 99.73, time: 433.4s\n",
      "[Epoch 6] Valid loss: 0.113, acc: 91.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1da58a9050426fbb8ee5233092d070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 0.001, acc: 99.88, time: 432.9s\n",
      "[Epoch 7] Valid loss: 0.108, acc: 92.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fea51416024b83af4689a173305b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 0.000, acc: 99.94, time: 433.9s\n",
      "[Epoch 8] Valid loss: 0.119, acc: 92.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f538c5739e413e885688383a2283b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.000, acc: 99.97, time: 433.4s\n",
      "[Epoch 9] Valid loss: 0.125, acc: 92.28\n",
      "Training finish! Best validation accuracy: 92.28\n",
      "\n",
      "Start trining 5th model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6de97b072b4055a9f72338a985183e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train loss: 0.129, acc: 72.23, time: 433.1s\n",
      "[Epoch 0] Valid loss: 0.111, acc: 80.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23dc70674d5f49ce844b8a70b6293a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train loss: 0.058, acc: 91.21, time: 433.8s\n",
      "[Epoch 1] Valid loss: 0.059, acc: 90.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1427726c0d49e192c2e5bae894ba39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train loss: 0.022, acc: 97.11, time: 433.7s\n",
      "[Epoch 2] Valid loss: 0.081, acc: 90.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91de1b81b024466ead92f0b24a0f3f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train loss: 0.012, acc: 98.40, time: 433.1s\n",
      "[Epoch 3] Valid loss: 0.108, acc: 89.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f63eb76970f406aa44b5928a25c9435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train loss: 0.007, acc: 99.23, time: 434.3s\n",
      "[Epoch 4] Valid loss: 0.093, acc: 90.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a614fce24af4029846651c557849461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train loss: 0.003, acc: 99.56, time: 433.6s\n",
      "[Epoch 5] Valid loss: 0.134, acc: 90.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415bbbac308045e2b1c48f56b6a3c9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train loss: 0.002, acc: 99.73, time: 433.0s\n",
      "[Epoch 6] Valid loss: 0.129, acc: 90.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a01ef2e23654802bea0f57bcb7f6dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train loss: 0.001, acc: 99.83, time: 434.0s\n",
      "[Epoch 7] Valid loss: 0.120, acc: 91.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97f4c83c81c4167a5d4d2f044380e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train loss: 0.001, acc: 99.94, time: 433.5s\n",
      "[Epoch 8] Valid loss: 0.121, acc: 91.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504849d026fe4307a25cac4bae73d16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train loss: 0.000, acc: 99.99, time: 433.1s\n",
      "[Epoch 9] Valid loss: 0.126, acc: 91.68\n",
      "Training finish! Best validation accuracy: 91.68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(ensen_num):\n",
    "    train(i, num_epochs, lr, train_loader, val_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tR1IYqs4XogP",
   "metadata": {
    "id": "tR1IYqs4XogP"
   },
   "outputs": [],
   "source": [
    "def train_from_my_model(idx, num_epochs, lr, train_loader, val_loader, tokenizer,model_saved_name):\n",
    "    print(f\"Start trining on {model_saved_name} directory \")\n",
    "    model = Roberta.from_pretrained(os.path.join(save_dir, model_saved_name), model_name).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = transformers.get_scheduler(\"linear\",\n",
    "                                           optimizer=optimizer,\n",
    "                                           num_warmup_steps=num_epochs * len(train_loader) // 10,\n",
    "                                           num_training_steps=num_epochs * len(train_loader))\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc = train_epoch(epoch, model, train_loader, optimizer, scheduler)\n",
    "        val_acc, _ = validate(epoch, model, val_loader)\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "            if not os.path.exists( os.path.join(save_dir, f'{idx}') ):\n",
    "                os.makedirs( os.path.join(save_dir, f'{idx}') )\n",
    "\n",
    "            model_to_save.save_pretrained(os.path.join(save_dir, f'{idx}'))\n",
    "            \n",
    "    print(f\"Training finish! Best validation accuracy: {best_acc*100:.2f}\\n\")\n",
    "\n",
    "\n",
    "#train_from_my_model(0, num_epochs, lr, train_loader, val_loader, tokenizer,\"layer2_relu_0_epoch5_91.25_seed18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff8a3a",
   "metadata": {
    "id": "9eff8a3a"
   },
   "source": [
    "## Test models (validation: 93.31%)\n",
    "- 위의 6개의 모델을 앙상블 적용하여 최종모델 만들기 \n",
    "- 정확도가 87%이하인 모델은 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0p4UL4u1lA0H",
   "metadata": {
    "id": "0p4UL4u1lA0H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntesting\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "testing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "-SUJTZCrYM9n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 54265,
     "status": "error",
     "timestamp": 1638436162243,
     "user": {
      "displayName": "­안형서 / 학생 / 전기·정보공학부",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2m5GEkg1AKOub_GkVdF2Ujkorj8KoGoebIxj9=s64",
      "userId": "10372927598583850844"
     },
     "user_tz": -540
    },
    "id": "-SUJTZCrYM9n",
    "outputId": "8c47a514-0c03-428d-fb76-8cc1dd657e1c"
   },
   "outputs": [],
   "source": [
    "def validate_tqdm(epoch, model, val_loader, verbose=True):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    cor = 0\n",
    "    n_sample = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(val_loader):\n",
    "            item = {key: val.to(device) for key, val in data.items()}\n",
    "            target = target.to(device)\n",
    "\n",
    "            logits = model(**item)\n",
    "            loss = criterion(logits, target)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            pred_all.append(preds)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            cor += (preds == target).sum().item()\n",
    "            n_sample += len(target)\n",
    "\n",
    "    loss_avg = total_loss / n_sample\n",
    "    acc = cor / n_sample\n",
    "    pred_all = torch.cat(pred_all)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[Epoch {epoch}] Valid loss: {loss_avg:.3f}, acc: {acc*100:.2f}\")\n",
    "    return acc, pred_all\n",
    "\n",
    "def validate_mymodel(val_loader, answer, model_saved_name):\n",
    "    model = Roberta.from_pretrained(os.path.join(save_dir, model_saved_name), model_name)\n",
    "    model.to(device)\n",
    "    acc, pred_all = validate_tqdm('best', model, val_loader, verbose=False)\n",
    "    print(f\"Load {idx}th model (acc: {acc*100:.2f})\")\n",
    "\n",
    "answer = torch.tensor(val_dataset.labels)\n",
    "#validate_mymodel(val_loader, answer, \"0_91.51_6epoch_seed12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63f561ef",
   "metadata": {
    "id": "63f561ef"
   },
   "outputs": [],
   "source": [
    "def validate_ensemble(val_loader, answer, idx_max=10):\n",
    "    pred_ensemble = []\n",
    "    for idx in range(idx_max):\n",
    "        model = Roberta.from_pretrained(os.path.join(save_dir, f'{idx}'), model_name)\n",
    "        model.to(device)\n",
    "        acc, pred_all = validate('best', model, val_loader, verbose=False)\n",
    "        print(f\"Load {idx}th model (acc: {acc*100:.2f})\")\n",
    "        if acc >= 0.87:\n",
    "            pred_ensemble.append(pred_all)\n",
    "        \n",
    "    pred_ensemble = torch.stack(pred_ensemble, dim=-1).float()\n",
    "    pred_ensemble = (pred_ensemble.mean(-1) >= 0.5).long().to(answer.device)\n",
    "    acc_ensemble = (pred_ensemble == answer).sum() / len(answer)\n",
    "    print(f\"\\nEnsemble accuracy: {acc_ensemble*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47ecdd9f",
   "metadata": {
    "id": "47ecdd9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 0th model (acc: 92.28)\n",
      "Load 1th model (acc: 92.54)\n",
      "Load 2th model (acc: 92.11)\n",
      "Load 3th model (acc: 92.20)\n",
      "Load 4th model (acc: 92.28)\n",
      "Load 5th model (acc: 91.68)\n",
      "\n",
      "Ensemble accuracy: 93.31\n"
     ]
    }
   ],
   "source": [
    "answer = torch.tensor(val_dataset.labels)\n",
    "\n",
    "validate_ensemble(val_loader, answer, idx_max=ensen_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
