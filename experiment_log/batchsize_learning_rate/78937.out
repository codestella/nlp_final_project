euclid
./result/large_8e-06_b5
Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[Epoch 0] Train loss: 0.133, acc: 57.74, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 0] Valid loss: 0.102, acc: 76.00
[Epoch 1] Train loss: 0.079, acc: 82.62, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 1] Valid loss: 0.088, acc: 80.00
[Epoch 2] Train loss: 0.033, acc: 93.94, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 2] Valid loss: 0.108, acc: 82.00
[Epoch 3] Train loss: 0.012, acc: 97.82, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 3] Valid loss: 0.148, acc: 83.43
[Epoch 4] Train loss: 0.005, acc: 99.13, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 4] Valid loss: 0.162, acc: 83.29
[Epoch 5] Train loss: 0.004, acc: 99.37, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 5] Valid loss: 0.164, acc: 82.86
[Epoch 6] Train loss: 0.002, acc: 99.73, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 6] Valid loss: 0.176, acc: 84.86
[Epoch 7] Train loss: 0.001, acc: 99.84, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 7] Valid loss: 0.181, acc: 84.14
[Epoch 8] Train loss: 0.001, acc: 99.92, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 8] Valid loss: 0.192, acc: 83.43
[Epoch 9] Train loss: 0.000, acc: 99.95, lr: {optimizer.param_groups[0]['lr']:.6f} time: {time.time()-s:.1f}s
[Epoch 9] Valid loss: 0.197, acc: 83.43
hostname : euclid, partition : titan, script : ~/anaconda3/envs/10/bin/python -u main.py  --size large --lr 8e-06 --batch_size 5
